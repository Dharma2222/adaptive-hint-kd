{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vpVs9RM0vHj"
      },
      "source": [
        "# CSCI-5501 – Deep Learning Applications - Assignment 2\n",
        "## \n",
        "## Deadline June 22, 2025\n",
        "### Total points: 100\n",
        "\n",
        "## Instructions\n",
        "* This is an individual assignment\n",
        "* All your solution, code, analysis, graphs, explanations should be done in this same notebook.\n",
        "* Please attempt to solve these questions by yourself. You can read official pytorch documentation and online resources to build your understanding, but please refrain from using LLMs (e.g., ChatGPT) to directly generate the answers.\n",
        "* Please make sure to execute all the cells before you generate the pdf and also the notebook submission on Crowdmark. You will not get points for the plots if they are not generated already.\n",
        "* **IMPORTANT:** Read every cell extremely carefully and attempt to understand the code clearly. Make note of any questions and bring to the next tutorial/TA-office-hours for further discussion and clarification.\n",
        "* Note: This notebook includes results corresponding to completed/correct implementation. All of the outputs are not guaranteed to be exactly same across different runs; however, these outputs should give you a sense of whether your implementation is working as expected.\n",
        "* Utilize Google Colab for training the models as it is compute intensive for CPU based traninig.\n",
        "\n",
        "## Learning Goals\n",
        "* Familiarize yourself with PyTorch.\n",
        "* Implement and analyse Fully-connected networks and convolutional networks.\n",
        "* Transfer-learning by fine-tuning a pretrained ImageNet classifier on CIFAR10.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ibSPcbwwspi2"
      },
      "outputs": [],
      "source": [
        "import random````````````````````````````````````````````````````````````````````````````````````````````\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms, datasets, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import copy\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using CUDA:\", torch.cuda.get_device_name(0))# if on Colab make sure this is running.\n",
        "elif torch.backends.mps.is_available():\n",
        "    device = torch.device(\"mps\")\n",
        "    print(\"Using Apple MPS (Metal Performance Shaders)\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3icNELkeExrn"
      },
      "source": [
        "# A. Fully-Connected Networks (10 points)\n",
        "\n",
        "In the following, we will define and train a Fully-Connected Classifier. Some parts of the function are filled out for you.\n",
        "Your tasks in this section are as follows:\n",
        "\n",
        "**A.1. [5 points]** Create the layers of fully-connected network.\n",
        "\n",
        "**A.2. [5 points]** Fill out the forward method. Remember to flatten the input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHZNDwEd0G-n"
      },
      "outputs": [],
      "source": [
        "class FullyConnectedClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    A fully-connected neural network for image classification.\n",
        "\n",
        "    Args:\n",
        "        image_size (tuple): Size of the input images (C, H, W) or (H, W).\n",
        "        hidden_units (list): List of integers specifying the number of hidden units in each layer.\n",
        "        activation_fn (nn.Module): Activation function to use between layers (e.g., nn.ReLU()).\n",
        "        num_classes (int): Number of output classes.\n",
        "        device (str): Device to use for training and evaluation (e.g., 'cpu' or 'cuda').\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_size, hidden_units=[128, 64], activation_fn=nn.ReLU(), num_classes=10, device='cpu'):\n",
        "        super(FullyConnectedClassifier, self).__init__()\n",
        "\n",
        "        ##########################################################################\n",
        "        ########################          A.1.           #########################\n",
        "        ######################## COMPLETE THIS FUNCTION. #########################\n",
        "        ##########################################################################\n",
        "\n",
        "\n",
        "        self.input_size = image_size[0] * image_size[1] * (image_size[2] if len(image_size) == 3 else 1)\n",
        "        layers = []\n",
        "        in_features = self.input_size\n",
        "        for units in hidden_units:\n",
        "            \"\"\"\n",
        "            Write code here.\n",
        "            \"\"\"\n",
        "            layers.append(nn.Linear(in_features, units))\n",
        "            layers.append(activation_fn)\n",
        "            in_features = units\n",
        "\n",
        "        layers.append(nn.Linear(in_features, num_classes))\n",
        "\n",
        "        # Convert the list to ModuleList to register the parameters.\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "        # Store the activation function.\n",
        "        self.activation_fn = activation_fn\n",
        "\n",
        "        # To enable GPU-usage.\n",
        "        self.device = device\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Apply the classifier.\n",
        "        # Hint: Every linear layer is followed by non-linearity except the final one\n",
        "        Input: x\n",
        "        Returns: logits.\n",
        "        \"\"\"\n",
        "        ##########################################################################\n",
        "        ########################          A.2.           #########################\n",
        "        ######################## FILL OUT THIS FUNCTION. #########################\n",
        "        ##########################################################################\n",
        "                 \n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x  # Return the raw logits\n",
        "\n",
        "        \"\"\"\n",
        "        Write code here.\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-hgfVfefZzf"
      },
      "source": [
        "\n",
        "\n",
        "# B. Convolutional Networks (20 points)\n",
        "\n",
        "In the following, we will define and train a Convolutional Classifier Network. Some parts of the function are filled out for you.\n",
        "Your tasks in this section are as follows:\n",
        "\n",
        "**B.1. [5 points]** The below table shows the sequence of transformations to be applied on an input image. **Fill out the shape after each transformation labelled with ??**\n",
        "\n",
        "**B.2. [10 points]** Fill out the constructor to initialize the network.\n",
        "\n",
        "**B.3. [5 points]** Fill out the forward method to apply the convolutional-network to the inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "JqQ4C8BEFniR",
        "outputId": "bc1e8548-0147-46dc-fa88-96f5098e8b8a"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.13.2' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "#############################################################################\n",
        "########################          B.1.           ############################\n",
        "######################## COMPUTE THE OUTPUT SHAPES. #########################\n",
        "#############################################################################\n",
        "\n",
        "architecture = [\n",
        "    {\n",
        "        \"Layer Type\": \"Input\",\n",
        "        \"Description\": \"Input image\",\n",
        "        \"Output Shape (HxWxC)\": \"(32, 32, 3)\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"Conv2D\",\n",
        "        \"Description\": \"12 filters, 3x3 kernel\",\n",
        "        \"Output Shape (HxWxC)\": \"(32, 32, 12)\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"BatchNorm\",\n",
        "        \"Description\": \"Normalize 12 channels\",\n",
        "        \"Output Shape (HxWxC)\": \"(32, 32, 12)\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"Activation\",\n",
        "        \"Description\": \"ReLU/LeakyReLU/etc\",\n",
        "        \"Output Shape (HxWxC)\": \"(32, 32, 12)\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"MaxPooling\",\n",
        "        \"Description\": \"2x2 pool, stride 2\",\n",
        "        \"Output Shape (HxWxC)\": \"(16, 16, 12)\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"Conv2D\",\n",
        "        \"Description\": \"24 filters, 3x3 kernel\",\n",
        "        \"Output Shape (HxWxC)\": \"(16, 16, 24)\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"BatchNorm\",\n",
        "        \"Description\": \"Normalize 24 channels\",\n",
        "        \"Output Shape (HxWxC)\": \"(16, 16, 24)\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"Activation\",\n",
        "        \"Description\": \"ReLU/LeakyReLU/etc\",\n",
        "        \"Output Shape (HxWxC)\": \"(16, 16, 24)\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"MaxPooling\",\n",
        "        \"Description\": \"2x2 pool, stride 2\",\n",
        "        \"Output Shape (HxWxC)\": \"(8, 8, 24)\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"Conv2D\",\n",
        "        \"Description\": \"48 filters, 3x3 kernel\",\n",
        "        \"Output Shape (HxWxC)\": \"(8, 8, 48)\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"BatchNorm\",\n",
        "        \"Description\": \"Normalize 48 channels\",\n",
        "        \"Output Shape (HxWxC)\": \"(8, 8, 48)\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"Activation\",\n",
        "        \"Description\": \"ReLU/LeakyReLU/etc\",\n",
        "        \"Output Shape (HxWxC)\": \"(8, 8, 48)\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"MaxPooling\",\n",
        "        \"Description\": \"2x2 pool, stride 2\",\n",
        "        \"Output Shape (HxWxC)\": \"(4, 4, 48)\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"Flatten\",\n",
        "        \"Description\": \"Flatten\",\n",
        "        \"Output Shape (HxWxC)\": \"768\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"Fully Connected\",\n",
        "        \"Description\": \"64 units\",\n",
        "        \"Output Shape (HxWxC)\": \"64\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"Activation\",\n",
        "        \"Description\": \"ReLU/LeakyReLU/etc\",\n",
        "        \"Output Shape (HxWxC)\": \"64\",\n",
        "    },\n",
        "    {\n",
        "        \"Layer Type\": \"Fully Connected\",\n",
        "        \"Description\": \"Output layer, 10 classes\",\n",
        "        \"Output Shape (HxWxC)\": \"10\",\n",
        "    }\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "# from google.colab import data_table # Turn on if error on Google Colab\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(architecture)\n",
        "df[[\"Layer Type\",\"Description\",\"Output Shape (HxWxC)\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZe5NVlH4gol"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple convolutional neural network for image classification.\n",
        "\n",
        "    Works with input images of size (32, 32, 3).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10, activation_fn=nn.ReLU, device='cpu'):\n",
        "        super(ConvNet, self).__init__()\n",
        "        ##########################################################################\n",
        "        ########################          B.2.           #########################\n",
        "        ######################## COMPLETE THIS FUNCTION. #########################\n",
        "        ##########################################################################\n",
        "\n",
        "    \n",
        "        self.classifier = nn.Sequential(\n",
        "            # Conv Block 1\n",
        "            nn.Conv2d(3, 12, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(12),\n",
        "            activation_fn(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            \n",
        "            # Conv Block 2\n",
        "            nn.Conv2d(12, 24, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(24),\n",
        "            activation_fn(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # (8, 8, 24)\n",
        "\n",
        "            # Conv Block 3\n",
        "            nn.Conv2d(24, 48, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(48),\n",
        "            activation_fn(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # (4, 4, 48)\n",
        "\n",
        "            \"\"\"\n",
        "            Write code here.\n",
        "            \"\"\"\n",
        "\n",
        "            nn.Flatten(),\n",
        "            \n",
        "            nn.Linear(768, 64),\n",
        "            activation_fn(),\n",
        "            \"\"\"\n",
        "            Write code here.\n",
        "            \"\"\"\n",
        "\n",
        "            nn.Linear(64, num_classes)\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        # To enable GPU-usage.\n",
        "        self.device = device\n",
        "        self.to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Apply the classifier.\n",
        "\n",
        "        Input: x\n",
        "        Returns: logits.\n",
        "        \"\"\"\n",
        "        ##########################################################################\n",
        "        ########################          B.3.           #########################\n",
        "        ######################## FILL OUT THIS FUNCTION. #########################\n",
        "        ##########################################################################\n",
        "        \n",
        "        return self.classifier(x)\n",
        "     \n",
        "        \"\"\"\n",
        "        Write code here.\n",
        "        \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJHK07pNBN6u"
      },
      "source": [
        "# C. Define Trainer-class to train a classifier model. (20 Points)\n",
        "\n",
        "**C.1 [5 points]** Fill out the code required for **Training Loop**  \n",
        "\n",
        "**C.2 [5 points]** Fill out the code for the function **def accuracy(self, data_loader)** \n",
        "\n",
        "**C.3 [10 points]** Fill out the code required for **Early Stopping**  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VPVFS-CAzZK"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Trainer class to handle the training and evaluation of the model.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to be trained.\n",
        "        device (str): Device to use for training and evaluation (e.g., 'cpu' or 'cuda').\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, device='cpu'):\n",
        "        self.model = model\n",
        "        self.device = device\n",
        "\n",
        "    def train_model(self, train_loader, val_loader, epochs=10, lr=0.001, weight_decay=0.1, early_stopping_patience=3, verbose=True, plot_graph=True, plot_title=\"\"):\n",
        "        \"\"\"\n",
        "        Train the model.\n",
        "\n",
        "        Args:\n",
        "            train_loader (DataLoader): DataLoader for training data.\n",
        "            val_loader (DataLoader): DataLoader for validation data.\n",
        "            epochs (int): Number of training epochs.\n",
        "            lr (float): Learning rate for the optimizer.\n",
        "            weight_decay (float): Weight decay for the optimizer.\n",
        "            early_stopping_patience (int): Patience for early stopping based on validation loss.\n",
        "        \"\"\"\n",
        "        optimizer = optim.AdamW(self.model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        best_val_loss = float('inf')\n",
        "        best_model_params = None\n",
        "        patience_counter = 0\n",
        "\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "\n",
        "        for epoch in tqdm.notebook.tqdm(range(epochs)):\n",
        "            # Training loop\n",
        "            self.model.train()\n",
        "\n",
        "            train_loss = 0.0\n",
        "            for inputs, targets in train_loader:\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "                ##########################################################################\n",
        "                ########################   Complete the code     #########################\n",
        "                ########################   for training loop.    #########################\n",
        "                ##########################################################################\n",
        "                #pass the inputs through the model\n",
        "                #calculate the loss\n",
        "                #check the gradients and update the model parameters\n",
        "                #add the loss to the train_loss variable \n",
        "\n",
        "                optimizer.zero_grad()                     # Reset gradients\n",
        "                outputs = self.model(inputs)             # Forward pass\n",
        "                loss = criterion(outputs, targets)       # Compute loss\n",
        "                loss.backward()                          # Backpropagation\n",
        "                optimizer.step()                         # Update weights\n",
        "\n",
        "                train_loss += loss.item()                # Accumulate training loss\n",
        "\n",
        "            train_loss /= len(train_loader)\n",
        "            train_losses.append(train_loss)\n",
        "\n",
        "            \"\"\"\n",
        "            For this part you are required to implement a simple early stopping mechanism based on validation loss.\n",
        "\n",
        "            Objective: The objective is to prevent overfitting by stopping training when the validation loss does not improve for a certain number of epochs (known as patience).\n",
        "\n",
        "            ### Steps to Implement:\n",
        "\n",
        "            1. Track Validation Loss:  \n",
        "            After each training epoch, compute the validation loss using your model.  \n",
        "            - Implement the function:  \n",
        "                ```python\n",
        "                def accuracy(self, data_loader):\n",
        "                    # Your code to compute validation accuracy or loss\n",
        "                ```\n",
        "            - Use this function to evaluate the model on the validation set after each epoch.\n",
        "\n",
        "            2. Monitor Improvement:  \n",
        "            - If the current validation loss (`val_loss`) is lower than the best validation loss observed so far (`best_val_loss`), update `best_val_loss` and reset the patience counter (patience_counter) to zero.\n",
        "\n",
        "            3. Count Non-Improving Epochs**:  \n",
        "            - If the validation loss does not improve (i.e., `val_loss >= best_val_loss`), increment the patience counter (`patience_counter += 1`).\n",
        "            - If `patience_counter` reaches or exceeds the specified hyperparameter `patience` (e.g., `early_stopping_patience = 5`), stop training early. This means that the model has not improved for `patience` consecutive epochs.\n",
        "\n",
        "            REMINDER:\n",
        "            - You are only required to stop training based on validation loss.\n",
        "            - Make sure your code prints out when early stopping is triggered\n",
        "            \"\"\"\n",
        "            # Validation loop\n",
        "            val_acc, val_loss = self.accuracy(val_loader)# you are required to implement this function.\n",
        "            val_losses.append(val_loss)\n",
        "            if verbose:\n",
        "              print(f\"Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f} Val Acc: {val_acc:.2f}\")\n",
        "\n",
        "            ##########################################################################\n",
        "            ########################   Complete the code  ############################\n",
        "            ########################   Early Stopping.    ############################\n",
        "            ##########################################################################\n",
        "            \n",
        "            \n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                # Update best validation loss and reset patience counter\n",
        "                best_model_params = copy.deepcopy(self.model.state_dict())\n",
        "                patience_counter = 0\n",
        "                if verbose:\n",
        "                  print(f\"Best val-loss updated: {best_val_loss}\")\n",
        "            else:\n",
        "                # patience counter and \n",
        "                if verbose:\n",
        "                  print(f\"Early stopping {patience_counter} of {early_stopping_patience}.\")\n",
        "                #check for early stopping\n",
        "\n",
        "        final_model_params = copy.deepcopy(self.model.state_dict())\n",
        "\n",
        "        # Plot losses\n",
        "        if plot_graph:\n",
        "          plt.figure(figsize=(10, 5))\n",
        "          plt.plot(train_losses, label='Train Loss')\n",
        "          plt.plot(val_losses, label='Validation Loss')\n",
        "          plt.xlabel('Epochs')\n",
        "          plt.ylabel('Loss')\n",
        "          plt.title(plot_title)\n",
        "          plt.legend()\n",
        "          plt.show()\n",
        "        return best_model_params, final_model_params\n",
        "\n",
        "    def accuracy(self, data_loader):\n",
        "        \"\"\"\n",
        "        Compute accuracy on a given DataLoader.\n",
        "\n",
        "        Args:\n",
        "            data_loader: for which the accuracy is to be evaluated.\n",
        "\n",
        "        Returns:\n",
        "            accuracy: percentage accuracy.\n",
        "            loss: CE loss.\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "        criterion = nn.CrossEntropyLoss(reduction='sum')\n",
        "        correct = 0\n",
        "        loss = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in data_loader:\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "                ##########################################################################\n",
        "                ########################   Complete the code     #########################\n",
        "                ########################   for evaluation loop.    #######################\n",
        "                ##########################################################################\n",
        "                #pass the inputs through the model to get raw logit predictions\n",
        "                #compute the loss using the criterion\n",
        "                #get the predicted classes by taking the argmax of the raw logit predictions https://docs.pytorch.org/docs/main/generated/torch.argmax.html\n",
        "                #increase the total number of samples counter (you can use pre built-in functions e.g size or shape https://docs.pytorch.org/docs/stable/size.html)\n",
        "                #calculate the number of correct predictions (you can use pre built-in functions e.g sum https://docs.pytorch.org/docs/stable/generated/torch.sum.html)\n",
        "                \n",
        "                outputs = self.model(inputs)\n",
        "                loss += criterion(outputs, targets).item()\n",
        "                predicted = torch.argmax(outputs, dim=1)\n",
        "                total += targets.size(0)\n",
        "                correct += (predicted == targets).sum().item()\n",
        "\n",
        "\n",
        "        return 100 * correct / total, loss/total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrCeY-49GJlc"
      },
      "source": [
        "# D. Helper code to create models and define datasets (20 points)\n",
        "\n",
        "#### **All code in this section has been written for you**\n",
        " \n",
        "### D.1 Reading comprehension questions\n",
        "\n",
        "Read the provided `train_model` function carefully and answer the following questions. You may refer to external documentation, blogs, or tutorials while answering — if you do, please include links.\n",
        "\n",
        "* These question are designed to help you clearly understand the rationale behind the code implementatiom\n",
        "* Answer each question as clearly and concisely as possible. You are encouraged to reference official PyTorch documentation or other reliable sources.\n",
        "---\n",
        "\n",
        "### Dataset Preparation\n",
        "\n",
        "1. Why do we apply `RandomHorizontalFlip()` to CIFAR-10 but not to MNIST?  \n",
        "\n",
        "Answer: CIFAR-10 contains images of objects like animals and vehicles, where flipping them horizontally still shows the same object. But for MNIST digits, flipping (like flipping a       \"6\" or \"2\") could change it into a different or unreadable digit, so we avoid flipping MNIST images.\n",
        "\n",
        "2. What is the purpose of `RandomCrop(32, padding=4)` in the CIFAR-10 preprocessing?  \n",
        "\n",
        "Answer: This operation adds padding around the image and then randomly crops it back to 32×32. It mimics small shifts or movements of objects in the image, which helps the model handle slight variations in position.\n",
        "\n",
        "3. Why is `RandomRotation(10)` used for MNIST? What kind of variability does it introduce? \n",
        "\n",
        "Answer: Handwritten digits may be slightly slanted in real-life writing. RandomRotation introduces minor angle variations to help the model become more tolerant to rotated digits.\n",
        "\n",
        "4. Explain why MNIST normalization uses `raw_train_dataset.data.float().mean() / 255` whereas CIFAR-10 normalization uses `raw_train_dataset.data.mean(axis=(0, 1, 2)) / 255`.  \n",
        "\n",
        "Answer: MNIST has only one grayscale channel, so we take the mean of all pixels. CIFAR-10 has three color channels (Red, Green, Blue), so we calculate the average for each channel separately to normalize the colors properly.\n",
        "\n",
        "5. What is the role of the `val_test_transform` pipeline? Why don't we apply any data augmentation there?\n",
        "\n",
        "Answer: This transformation is used for validation and test data. We skip augmentations here because we want to measure how well the model performs on clean, unaltered data — not on modified versions.\n",
        "\n",
        "---\n",
        "\n",
        "### Dataset Loading and Splitting\n",
        "\n",
        "6. Why do we download the dataset again with different transformations (`train_dataset` vs `train_dataset_val`)?  \n",
        "\n",
        "Answer: We use different transformations for training and validation. So we load the dataset twice — once with augmentation for training, and once without for validation.\n",
        "\n",
        "7. What is the purpose of calling `random_split` twice on `train_dataset` and `train_dataset_val`?  \n",
        "\n",
        "Answer: We want to compare the same data samples under two different settings: one with augmentation (for training) and one without (for validation). Splitting both ensures they contain matching samples.\n",
        "\n",
        "8. Why do we use `torch.Generator().manual_seed(0)` in `random_split()`? What problem does it prevent?\n",
        " \n",
        "Answer:Setting the random seed ensures the split is the same every time the code runs. This helps keep results consistent and makes debugging or comparing models easier.\n",
        "\n",
        "---\n",
        "\n",
        "### DataLoader Setup\n",
        "\n",
        "9. Why is `shuffle=True` used for `train_loader` but not for `val_loader` and `test_loader`?  \n",
        "\n",
        "Answer: Shuffling the training data gives the model varied batches each time, which improves learning. We don’t shuffle validation or test data because we want repeatable, fair evaluations.\n",
        "\n",
        "10. What is the impact of batch size on the training process?\n",
        "\n",
        "Answer:Batch size controls how many samples are processed at once. Larger batches train faster but use more memory and may generalize less well. Smaller batches are slower but often help the model generalize better to new data.\n",
        "\n",
        "---\n",
        "\n",
        "### Training and Evaluation\n",
        "\n",
        "11. Why do we save both `best_model_params` and `final_model_params`?  \n",
        "\n",
        "Answer:We save the best model (based on validation loss) to use for evaluation. The final model might have overfitted, so saving both lets us choose the better one later.\n",
        "\n",
        "12. What function does `early_stopping_patience` serve?  \n",
        "\n",
        "Answer: It stops training if the model hasn't improved for a certain number of epochs. This helps avoid wasting time and also prevents the model from overfitting.\n",
        "\n",
        "13. The model is evaluated both on validation and test datasets. What is the difference between these two evaluations? \n",
        "\n",
        "Answer: Validation is used during training to fine-tune the model. The test set is only used at the end to see how well the model performs on completely unseen data.\n",
        "\n",
        "14. Why do we reload model parameters before each evaluation (using `model.load_state_dict(params)`)?\n",
        "\n",
        "Answer: To make sure we’re evaluating the exact version of the model we want — either the best or the final one. It avoids using any partially trained or outdated weights.\n",
        "\n",
        "---\n",
        "\n",
        "### Trainer Logic\n",
        "\n",
        "15. What is the responsibility of the `Trainer` class? Which parts of the training pipeline does it abstract away?\n",
        "\n",
        "Answer: The Trainer class takes care of training, validation, early stopping, and loss tracking. It simplifies the entire training process so that you don’t have to write the same code again for each new model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLU3qgXoGI9N"
      },
      "outputs": [],
      "source": [
        "class ModelFactory:\n",
        "    \"\"\"\n",
        "    Model factory class.\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def create_model(model_type: str, **kwargs) -> nn.Module:\n",
        "        \"\"\"\n",
        "        Create a model instance based on the specified type.\n",
        "\n",
        "        Args:\n",
        "            model_type (str): The type of model to create. Must be one of [\"ConvNet\", \"fully_connected\"].\n",
        "            **kwargs: Additional arguments required for model initialization.\n",
        "\n",
        "        Returns:\n",
        "            nn.Module: An instance of the requested model.\n",
        "        \"\"\"\n",
        "        model_type = model_type.lower()\n",
        "        if model_type == \"convnet\":\n",
        "            return ConvNet(**kwargs)\n",
        "        elif model_type == \"fully_connected\":\n",
        "            return FullyConnectedClassifier(**kwargs)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown model {model_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybDZ9vB2wxlO"
      },
      "outputs": [],
      "source": [
        "def train_model(model=None,\n",
        "                dataset_name=\"cifar10\",\n",
        "                epochs=20,\n",
        "                lr=0.001,\n",
        "                weight_decay=0.1,\n",
        "                early_stopping_patience=float('inf'),\n",
        "                mean_std = None,\n",
        "                plot_graph=True,\n",
        "                verbose=True):\n",
        "    if dataset_name.lower() == \"mnist\":\n",
        "        # MNIST preprocessing\n",
        "        raw_train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "        image_size = (1, 28, 28)\n",
        "        num_classes = 10\n",
        "        if mean_std is not None:\n",
        "          mean, std = mean_std\n",
        "        else:\n",
        "          mean = raw_train_dataset.data.float().mean() / 255\n",
        "          std = raw_train_dataset.data.float().std() / 255\n",
        "\n",
        "        # Train transformation specific to MNIST\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.RandomRotation(10),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "    elif dataset_name.lower() == \"cifar10\":\n",
        "        # CIFAR-10 preprocessing\n",
        "\n",
        "        raw_train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "        image_size = (3, 32, 32)\n",
        "        num_classes = 10\n",
        "        if mean_std is not None:\n",
        "          mean, std = mean_std\n",
        "        else:\n",
        "          mean = raw_train_dataset.data.mean(axis=(0, 1, 2)) / 255\n",
        "          std = raw_train_dataset.data.std(axis=(0, 1, 2)) / 255\n",
        "\n",
        "        # Train transformation specific to CIFAR-10\n",
        "        train_transform = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.RandomCrop(32, padding=4),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "\n",
        "    val_test_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std)\n",
        "        ])\n",
        "\n",
        "    # Load datasets\n",
        "    if dataset_name.lower() == \"mnist\":\n",
        "        train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=train_transform)\n",
        "        train_dataset_val = datasets.MNIST(root='./data', train=True, download=True, transform=val_test_transform)\n",
        "        test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=val_test_transform)\n",
        "    else:\n",
        "        train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "        train_dataset_val = datasets.CIFAR10(root='./data', train=True, download=True, transform=val_test_transform)\n",
        "        test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=val_test_transform)\n",
        "\n",
        "    # Split train dataset into training and validation\n",
        "    val_size = 10000\n",
        "    train_size = len(train_dataset) - val_size\n",
        "    train_subset, _ = random_split(train_dataset, [train_size, val_size],generator=torch.Generator().manual_seed(0))\n",
        "    _, val_subset = random_split(train_dataset_val, [train_size, val_size],generator=torch.Generator().manual_seed(0))\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
        "    val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    # Initialize the Trainer\n",
        "    trainer = Trainer(model, device=model.device)\n",
        "\n",
        "    # Train the model\n",
        "    best_model_params, final_model_params = trainer.train_model(train_loader,\n",
        "                                                                val_loader,\n",
        "                                                                epochs=epochs,\n",
        "                                                                lr=lr,\n",
        "                                                                weight_decay=weight_decay,\n",
        "                                                                early_stopping_patience=early_stopping_patience,\n",
        "                                                                verbose=verbose,\n",
        "                                                                plot_graph=plot_graph,\n",
        "                                                                plot_title=f\"{str(type(model).__name__)} : {dataset_name.upper()}\"\n",
        "                                                                )\n",
        "    results = dict()\n",
        "    for description, params in  zip([\"Best-checkpoint\", \"Final-checkpoint\"],[best_model_params, final_model_params]):\n",
        "      print(f\"Evaluating {description}\")\n",
        "      model.load_state_dict(params)\n",
        "\n",
        "      val_accuracy, val_loss = trainer.accuracy(val_loader)\n",
        "      print(f\"\\tValidation Accuracy: {val_accuracy:.2f}%\")\n",
        "      print(f\"\\tValidation loss: {val_loss:.4f}\")\n",
        "\n",
        "      print()\n",
        "\n",
        "      test_accuracy, test_loss = trainer.accuracy(test_loader)\n",
        "      print(f\"\\tTest Accuracy: {test_accuracy:.2f}%\")\n",
        "      print(f\"\\tTest loss: {test_loss:.4f}\")\n",
        "\n",
        "      results[f\"{description}:state_dict\"] = params\n",
        "      results[f\"{description}:val_loss\"] = val_loss\n",
        "      results[f\"{description}:val_acc\"] = val_accuracy\n",
        "      results[f\"{description}:test_loss\"] = test_loss\n",
        "      results[f\"{description}:test_acc\"] = test_accuracy\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-Cf4aauz90l"
      },
      "source": [
        "# E. Train Models (15 points)\n",
        "\n",
        "**All code has been written in this section.**\n",
        "\n",
        "**E.1 [15 points]** Run the following cells and provide an analysis comparing between the architectures and datasets.\n",
        "\n",
        "**Answer:** \n",
        " | Model | Dataset  | Best Validation Accuracy | Best Test Accuracy |\n",
        "| ----- | -------- | ------------------------ | ------------------ |\n",
        "| FCN   | MNIST    | **98.52%**               | **98.41%**         |\n",
        "| FCN   | CIFAR-10 | **44.75%**               | **45.55%**         |\n",
        "| CNN   | CIFAR-10 | **76.94%**               | **77.36%**         |\n",
        "\n",
        "1. Dataset Complexity Affects Architecture Suitability\n",
        "MNIST consists of simple, grayscale digit images (28×28). The shapes are centered and don’t require spatial feature recognition. Because of this, a fully connected model performs exceptionally well — achieving over 98% accuracy without the need for convolutional operations.\n",
        "\n",
        "CIFAR-10, on the other hand, is much more complex. It contains color images (32×32 RGB) of real-world objects with varying backgrounds, positions, and textures. Fully connected layers flatten the image, discarding the layout of pixels, which makes them ill-suited for this type of data. The resulting performance drops to around 45%.\n",
        "\n",
        "\n",
        "2. CNNs Handle Image Structure Better\n",
        "CNNs preserve the two-dimensional structure of the image and use filters to detect patterns like edges, corners, and textures. This makes them ideal for handling visual features.\n",
        "\n",
        "On CIFAR-10, the convolutional model greatly outperforms the FCN, achieving 77.36% test accuracy. This confirms the importance of spatial awareness when dealing with more realistic image data.\n",
        "\n",
        "3. Validation and Loss Comparison\n",
        "| Model     | Best Val Loss | Best Test Loss |\n",
        "| --------- | ------------- | -------------- |\n",
        "| MNIST FCN | 0.0550        | 0.0554         |\n",
        "| CIFAR FCN | 1.5370        | 1.5091         |\n",
        "| CIFAR CNN | 0.6685        | 0.6681         |\n",
        "\n",
        "The FCN performs well on MNIST because of the simplicity of the data — leading to very low loss values.\n",
        "\n",
        "On CIFAR-10, the FCN struggles with high loss, showing that it's not able to generalize effectively.\n",
        "\n",
        "The CNN achieves much lower loss on CIFAR-10, reflecting more accurate predictions.\n",
        "\n",
        "4. Stability of Training\n",
        "The difference between the best and final checkpoints is small across all experiments, suggesting that the training process was relatively stable and that the early stopping mechanism worked effectively in most cases.\n",
        "\n",
        "\n",
        "Conclution\n",
        "\n",
        "Fully connected networks are efficient and effective for simple datasets like MNIST, where spatial relationships are not critical.\n",
        "\n",
        "Convolutional networks are far better suited for image classification tasks that involve complex visuals, like those in CIFAR-10.\n",
        "\n",
        "For real-world image data, using CNNs is essential to achieve strong accuracy and lower loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 727,
          "referenced_widgets": [
            "9f1700bc056e4448a3676283f5afa246",
            "c2319810e551414f8efcaafb36cd38ce",
            "922e2a514e1240648fef93834032eeaa",
            "d0eccd226cd644f6a9c9535f57520828",
            "4d4604bd900441af90454e07c2e04ca8",
            "862794c1d5ad4af2922e8102bb8a04ec",
            "9504eb8cdbd642c28f594ad7aad1ee83",
            "06debbed3b304505ae32e986635e538c",
            "2a077a96e6174887a9bea3f076b95425",
            "1bedba4c3fc44f939570a8792fc988db",
            "d81c464b597a4758b00ba4573b4ae642"
          ]
        },
        "id": "6j9PVJps7g1C",
        "outputId": "ef0b6038-ead5-463e-d00e-0be796632539"
      },
      "outputs": [],
      "source": [
        "mnist_FC = ModelFactory.create_model(\n",
        "    model_type=\"fully_connected\",\n",
        "    image_size=(1, 28, 28),\n",
        "    hidden_units=[512,256],\n",
        "    activation_fn=nn.SiLU(),\n",
        "    num_classes=10,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "_ = train_model(dataset_name=\"mnist\", model=mnist_FC, epochs=50, early_stopping_patience=float('inf'),verbose=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797,
          "referenced_widgets": [
            "3c4b66b5f9c749e7847af0ed7a39a861",
            "7109e8496a0a42eb943a3af379ef4d33",
            "15714190e5474bbe8c8701cb7863ecc8",
            "0b409020dfa4465686dcabcf3b15c909",
            "1d392b8b85644474ab8b74ec1ee94bef",
            "74c48db2d1a142abb0ded8a9c3c03748",
            "816812a5c86c43e1810fd2e7e845e147",
            "aada10250819487aab15834c17511fcb",
            "d236fd7e2b824df1af0f2c1e79106167",
            "6d564833558747baa737f88c84528eee",
            "fcd0ea84eb814b328367aa83a594dbc3"
          ]
        },
        "id": "OId2qEiaTZkr",
        "outputId": "32e0696f-1007-4af0-f5b6-c998aa2d31ef"
      },
      "outputs": [],
      "source": [
        "cifar_FC = ModelFactory.create_model(\n",
        "    model_type=\"fully_connected\",\n",
        "    image_size=(3, 32, 32),\n",
        "    hidden_units=[1024,512,256],\n",
        "    activation_fn=nn.SiLU(),\n",
        "    num_classes=10,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "_ = train_model(dataset_name=\"cifar10\",model=cifar_FC, epochs=50, early_stopping_patience=float('inf'),verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797,
          "referenced_widgets": [
            "fce1c9b9bd7c40d3939b632e2939df92",
            "d2da2ead9d1045b58a32f810de49ebd6",
            "191e2f853fdd41f0b85aff39fc304ef6",
            "1213a2de77f242feb9fc6a1ada6c1fee",
            "b9e7aee659ae4cdeaec2ad47eafc6a54",
            "20c3120b0ae1420e9654795e0f6a2269",
            "635274f07a544c168c25a52ebff85464",
            "3ed5148c207a496a9b0f3b0cbd145998",
            "02fdf97ea140408093c1caa1f8c0ce7e",
            "2c192fc147a6410c8a5cec4265d6431b",
            "d315b831d76347a3a46bc61c0a162487"
          ]
        },
        "id": "i4jrmIA9fZWp",
        "outputId": "3df32f9b-adcf-4f8d-a754-67cf4983c4ac"
      },
      "outputs": [],
      "source": [
        "cifar_CNN = ModelFactory.create_model(\n",
        "    model_type=\"convnet\",\n",
        "    activation_fn=nn.SiLU,\n",
        "    num_classes=10,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "_ = train_model(dataset_name=\"cifar10\",model=cifar_CNN, epochs=50, early_stopping_patience=float('inf'),verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBZeuAhVfSz1"
      },
      "source": [
        "# F. Fine-tuning Pretrained ResNet-18 model (15 points)\n",
        "\n",
        "In this section, we will take a ResNet-18 model pretrained on ImageNet and fine-tune it on CIFAR10. When fine-tuning a pre-trained model on a downstream task, only a subset of parameters are adapated and remaining are kept frozen.\n",
        "The ResNet18 model architecture is shown below. At a high-level, the architecture can be categorized into input-convolution (conv1/bn1), layer1, layer2, layer3, layer4 and fc. To adapt ResNet18 to CIFAR10, we have two steps:\n",
        "* Firstly, we replace fc with a new linear-layer that maps to 10 classes instead of 1000 classes.\n",
        "* Then, we fine-tune this network on CIFAR10 **for 1 epoch** and experiment with freezing different layers of this network.\n",
        "\n",
        "\n",
        "```\n",
        "ResNet(\n",
        "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "  (relu): ReLU(inplace=True)\n",
        "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
        "  (layer1): Sequential(\n",
        "    (0): BasicBlock(\n",
        "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (relu): ReLU(inplace=True)\n",
        "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    )\n",
        "    (1): BasicBlock(\n",
        "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (relu): ReLU(inplace=True)\n",
        "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    )\n",
        "  )\n",
        "  (layer2): Sequential(\n",
        "    (0): BasicBlock(\n",
        "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (relu): ReLU(inplace=True)\n",
        "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (downsample): Sequential(\n",
        "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
        "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      )\n",
        "    )\n",
        "    (1): BasicBlock(\n",
        "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (relu): ReLU(inplace=True)\n",
        "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    )\n",
        "  )\n",
        "  (layer3): Sequential(\n",
        "    (0): BasicBlock(\n",
        "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (relu): ReLU(inplace=True)\n",
        "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (downsample): Sequential(\n",
        "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
        "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      )\n",
        "    )\n",
        "    (1): BasicBlock(\n",
        "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (relu): ReLU(inplace=True)\n",
        "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    )\n",
        "  )\n",
        "  (layer4): Sequential(\n",
        "    (0): BasicBlock(\n",
        "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
        "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (relu): ReLU(inplace=True)\n",
        "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (downsample): Sequential(\n",
        "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
        "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      )\n",
        "    )\n",
        "    (1): BasicBlock(\n",
        "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "      (relu): ReLU(inplace=True)\n",
        "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
        "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
        "    )\n",
        "  )\n",
        "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
        ")\n",
        "```\n",
        "\n",
        "**All code has been written in this section.**\n",
        "\n",
        "**F.1 [15 points]** Run the following cells and provide an analysis connecting the configuration of frozen-layers and observed validation/test accuracies.\n",
        "\n",
        "**Answer:**\n",
        "1. Freezing only early layers (conv1, bn1) gives the best performance\n",
        "Freezing just the very first convolution and batch norm layers led to almost 89% accuracy, the highest observed in the experiment. These early layers typically extract generic low-level features (like edges and textures), which are already well-learned from ImageNet. Reusing them helps the model focus on learning dataset-specific features in deeper layers, speeding up convergence and improving performance.\n",
        "\n",
        "2. Gradual freezing of more layers gives diminishing returns\n",
        "When more layers were frozen — layer1, layer2, then layer3 — the model still performed well but with slightly reduced accuracy. This shows that mid-level and high-level layers do benefit from fine-tuning, since CIFAR-10 classes differ from ImageNet (e.g., airplanes, frogs, trucks). These deeper layers need adaptation to match the target domain.\n",
        "\n",
        "3. Freezing all residual blocks (layer1–4) significantly hurts performance\n",
        "With almost the entire ResNet backbone frozen, only the final classifier is trainable. This led to only 78% accuracy, nearly equal to the baseline where all layers were trained from scratch. It suggests that too much reliance on pretrained weights limits the model’s ability to adapt to new patterns in CIFAR-10.\n",
        "\n",
        "4. Training all layers from scratch is not optimal\n",
        "Interestingly, not freezing anything (training the whole model) gave the lowest performance. This could be due to the model “forgetting” useful pretrained features in early epochs — a known issue in transfer learning when learning rates are not carefully tuned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux0R3LmOfYqJ"
      },
      "outputs": [],
      "source": [
        "class ResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10, freeze_blocks=None,device='cpu'):\n",
        "        super(ResNet18, self).__init__()\n",
        "        self.device = device\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        # Load pretrained ResNet-18 model from torchvision.models\n",
        "        # Read more here: https://pytorch.org/vision/main/models.html\n",
        "        self.model = models.resnet18(weights='DEFAULT')\n",
        "\n",
        "\n",
        "        # Modify the last fully connected layer to match the number of classes\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
        "\n",
        "        # Freeze certain blocks if specified\n",
        "        if freeze_blocks:\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if any(block in name for block in freeze_blocks):\n",
        "                    param.requires_grad = False\n",
        "\n",
        "        self.model = self.model.to(self.device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Resize images to 224x224 in the forward pass\n",
        "        x = F.interpolate(x, size=(224, 224), mode='bilinear')\n",
        "        return self.model(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o_cJiQUkTbXD"
      },
      "outputs": [],
      "source": [
        "freeze_blocks_options = [\n",
        "    None,  # Train all layers\n",
        "    [\"conv1\", \"bn1\"],  # Freeze initial conv/batchnorm\n",
        "    [\"layer1\"],  # Freeze first residual layer\n",
        "    [\"layer1\", \"layer2\"],  # Freeze first two residual layers\n",
        "    [\"layer1\", \"layer2\", \"layer3\"],  # Freeze first three residual layers\n",
        "    [\"layer1\", \"layer2\", \"layer3\", \"layer4\"],  # Freeze all four residual layers\n",
        "]\n",
        "for freeze_blocks in freeze_blocks_options:\n",
        "  print(f\"{'='*5} Fine-tuning ResNet18: {'Frozen-layers:'+','.join(freeze_blocks) if freeze_blocks else 'Training all layers'} {'='*5}\")\n",
        "  _ = train_model(\n",
        "      model = ResNet18(freeze_blocks=freeze_blocks,device=device),\n",
        "      dataset_name=\"cifar10\",\n",
        "      epochs = 1,\n",
        "      lr=0.001,\n",
        "      weight_decay=0.1,\n",
        "      early_stopping_patience=float('inf'),\n",
        "      # Normalize using ImageNet mean and std\n",
        "      # Read: https://pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html\n",
        "      mean_std = [(0.485, 0.456, 0.406), (0.229, 0.224, 0.225)],\n",
        "      verbose=False,\n",
        "      plot_graph=False\n",
        "  )\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02fdf97ea140408093c1caa1f8c0ce7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06debbed3b304505ae32e986635e538c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b409020dfa4465686dcabcf3b15c909": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d564833558747baa737f88c84528eee",
            "placeholder": "​",
            "style": "IPY_MODEL_fcd0ea84eb814b328367aa83a594dbc3",
            "value": " 50/50 [15:19&lt;00:00, 18.33s/it]"
          }
        },
        "1213a2de77f242feb9fc6a1ada6c1fee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c192fc147a6410c8a5cec4265d6431b",
            "placeholder": "​",
            "style": "IPY_MODEL_d315b831d76347a3a46bc61c0a162487",
            "value": " 50/50 [15:49&lt;00:00, 18.92s/it]"
          }
        },
        "15714190e5474bbe8c8701cb7863ecc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aada10250819487aab15834c17511fcb",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d236fd7e2b824df1af0f2c1e79106167",
            "value": 50
          }
        },
        "191e2f853fdd41f0b85aff39fc304ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ed5148c207a496a9b0f3b0cbd145998",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_02fdf97ea140408093c1caa1f8c0ce7e",
            "value": 50
          }
        },
        "1bedba4c3fc44f939570a8792fc988db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d392b8b85644474ab8b74ec1ee94bef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20c3120b0ae1420e9654795e0f6a2269": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a077a96e6174887a9bea3f076b95425": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c192fc147a6410c8a5cec4265d6431b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c4b66b5f9c749e7847af0ed7a39a861": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7109e8496a0a42eb943a3af379ef4d33",
              "IPY_MODEL_15714190e5474bbe8c8701cb7863ecc8",
              "IPY_MODEL_0b409020dfa4465686dcabcf3b15c909"
            ],
            "layout": "IPY_MODEL_1d392b8b85644474ab8b74ec1ee94bef"
          }
        },
        "3ed5148c207a496a9b0f3b0cbd145998": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d4604bd900441af90454e07c2e04ca8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "635274f07a544c168c25a52ebff85464": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d564833558747baa737f88c84528eee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7109e8496a0a42eb943a3af379ef4d33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74c48db2d1a142abb0ded8a9c3c03748",
            "placeholder": "​",
            "style": "IPY_MODEL_816812a5c86c43e1810fd2e7e845e147",
            "value": "100%"
          }
        },
        "74c48db2d1a142abb0ded8a9c3c03748": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "816812a5c86c43e1810fd2e7e845e147": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "862794c1d5ad4af2922e8102bb8a04ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "922e2a514e1240648fef93834032eeaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06debbed3b304505ae32e986635e538c",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a077a96e6174887a9bea3f076b95425",
            "value": 50
          }
        },
        "9504eb8cdbd642c28f594ad7aad1ee83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f1700bc056e4448a3676283f5afa246": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2319810e551414f8efcaafb36cd38ce",
              "IPY_MODEL_922e2a514e1240648fef93834032eeaa",
              "IPY_MODEL_d0eccd226cd644f6a9c9535f57520828"
            ],
            "layout": "IPY_MODEL_4d4604bd900441af90454e07c2e04ca8"
          }
        },
        "aada10250819487aab15834c17511fcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9e7aee659ae4cdeaec2ad47eafc6a54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2319810e551414f8efcaafb36cd38ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_862794c1d5ad4af2922e8102bb8a04ec",
            "placeholder": "​",
            "style": "IPY_MODEL_9504eb8cdbd642c28f594ad7aad1ee83",
            "value": "100%"
          }
        },
        "d0eccd226cd644f6a9c9535f57520828": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bedba4c3fc44f939570a8792fc988db",
            "placeholder": "​",
            "style": "IPY_MODEL_d81c464b597a4758b00ba4573b4ae642",
            "value": " 50/50 [12:49&lt;00:00, 15.36s/it]"
          }
        },
        "d236fd7e2b824df1af0f2c1e79106167": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2da2ead9d1045b58a32f810de49ebd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20c3120b0ae1420e9654795e0f6a2269",
            "placeholder": "​",
            "style": "IPY_MODEL_635274f07a544c168c25a52ebff85464",
            "value": "100%"
          }
        },
        "d315b831d76347a3a46bc61c0a162487": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d81c464b597a4758b00ba4573b4ae642": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcd0ea84eb814b328367aa83a594dbc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fce1c9b9bd7c40d3939b632e2939df92": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d2da2ead9d1045b58a32f810de49ebd6",
              "IPY_MODEL_191e2f853fdd41f0b85aff39fc304ef6",
              "IPY_MODEL_1213a2de77f242feb9fc6a1ada6c1fee"
            ],
            "layout": "IPY_MODEL_b9e7aee659ae4cdeaec2ad47eafc6a54"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
